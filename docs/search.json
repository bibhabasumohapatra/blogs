[
  {
    "objectID": "posts/linear-regression-from-scratch.html",
    "href": "posts/linear-regression-from-scratch.html",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "import torch\n\n\nx_train = torch.linspace(0, 1, 20)\ny_train = torch.sin(2*3.14*x_train) + torch.normal(0,0.2,x_train.shape,) ## added noise as per the book\n\n### testing data\nx_test = torch.linspace(0, 1, 200)\ny_test = torch.sin(2*3.14*x_test) \n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\nTo solve for the optimal weight matrix ( W ), given the expression ( (XW - Y)^T (XW - Y) ), we can proceed by first expanding the expression and then minimizing it with respect to ( W ).\n\n\nGiven: \\[\nL(W) = (XW - Y)^T (XW - Y)\n\\]\nExpanding this: \\[\nL(W) = (XW)^T(XW) - (XW)^T Y - Y^T (XW) + Y^T Y\n\\]\nSimplifying further: \\[\nL(W) = W^T X^T XW - W^T X^T Y - Y^T XW + Y^T Y\n\\]\nSince ( W^T X^T Y ) and ( Y^T XW ) are scalars, they are equal. Therefore: \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTo find the optimal ( W ), take the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2X^T XW - 2X^T Y = 0\n\\]\nSimplifying: \\[\nX^T XW = X^T Y\n\\]\nFinally, solve for ( W ): \\[\nW = (X^T X)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is given by: \\[\n\\boxed{W = (X^T X)^{-1} X^T Y}\n\\]\nThis result is commonly known as the Normal Equation in linear regression.\n\npowers = torch.arange(5).float()\nx_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\nw = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n\n\nw.shape, x_test.shape\n\n(torch.Size([5, 1]), torch.Size([200]))\n\n\n\nx_poly_test = x_test.unsqueeze(-1).pow(powers)\ny_pred = x_poly_test @ w\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\nplt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    w = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))\n\n\n\n\n\n\n\n\n\n\n\n\nOne technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified error function of the form.\nTechniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefficients. The particular case of a quadratic regularizer is called ridge regres\u0002sion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is known as weight decay\nTo solve for the optimal weight matrix ( W ) given the objective function ( (XW - Y)^T (XW - Y) + W^T W ), follow these steps:\n\n\nThe objective function is: \\[\nL(W) = (XW - Y)^T (XW - Y) + W^T W\n\\]\n\n\n\nExpanding ( (XW - Y)^T (XW - Y) ): \\[\n(XW - Y)^T (XW - Y) = (XW)^T XW - (XW)^T Y - Y^T XW + Y^T Y\n\\]\nSince ( (XW)^T Y ) and ( Y^T XW ) are scalars and equal, this simplifies to: \\[\n(XW - Y)^T (XW - Y) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\nAdding ( W^T W ): \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y + W^T W\n\\]\nCombining like terms: \\[\nL(W) = W^T (X^T X + I) W - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTake the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2(X^T X + I)W - 2X^T Y = 0\n\\]\nSolving for ( W ): \\[\n(X^T X + I)W = X^T Y\n\\] \\[\nW = (X^T X + I)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is: \\[\n\\boxed{W = (X^T X + I)^{-1} X^T Y}\n\\]\nThis result includes a regularization term ( W^T W ), which is common in Ridge Regression or Tikhonov Regularization.\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    N,M = x_poly.shape\n\n    w = (torch.linalg.inv(x_poly.T@x_poly + torch.eye(M))@x_poly.T) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#plot-training-data",
    "href": "posts/linear-regression-from-scratch.html#plot-training-data",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.scatter(x_train, y_train)"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#fit-the-linear-model-on-training-data",
    "href": "posts/linear-regression-from-scratch.html#fit-the-linear-model-on-training-data",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "To solve for the optimal weight matrix ( W ), given the expression ( (XW - Y)^T (XW - Y) ), we can proceed by first expanding the expression and then minimizing it with respect to ( W ).\n\n\nGiven: \\[\nL(W) = (XW - Y)^T (XW - Y)\n\\]\nExpanding this: \\[\nL(W) = (XW)^T(XW) - (XW)^T Y - Y^T (XW) + Y^T Y\n\\]\nSimplifying further: \\[\nL(W) = W^T X^T XW - W^T X^T Y - Y^T XW + Y^T Y\n\\]\nSince ( W^T X^T Y ) and ( Y^T XW ) are scalars, they are equal. Therefore: \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTo find the optimal ( W ), take the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2X^T XW - 2X^T Y = 0\n\\]\nSimplifying: \\[\nX^T XW = X^T Y\n\\]\nFinally, solve for ( W ): \\[\nW = (X^T X)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is given by: \\[\n\\boxed{W = (X^T X)^{-1} X^T Y}\n\\]\nThis result is commonly known as the Normal Equation in linear regression.\n\npowers = torch.arange(5).float()\nx_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\nw = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n\n\nw.shape, x_test.shape\n\n(torch.Size([5, 1]), torch.Size([200]))\n\n\n\nx_poly_test = x_test.unsqueeze(-1).pow(powers)\ny_pred = x_poly_test @ w\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\nplt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    w = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#example-polynomial-curve-fitting-with-regularization",
    "href": "posts/linear-regression-from-scratch.html#example-polynomial-curve-fitting-with-regularization",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "One technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified error function of the form.\nTechniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefficients. The particular case of a quadratic regularizer is called ridge regres\u0002sion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is known as weight decay\nTo solve for the optimal weight matrix ( W ) given the objective function ( (XW - Y)^T (XW - Y) + W^T W ), follow these steps:\n\n\nThe objective function is: \\[\nL(W) = (XW - Y)^T (XW - Y) + W^T W\n\\]\n\n\n\nExpanding ( (XW - Y)^T (XW - Y) ): \\[\n(XW - Y)^T (XW - Y) = (XW)^T XW - (XW)^T Y - Y^T XW + Y^T Y\n\\]\nSince ( (XW)^T Y ) and ( Y^T XW ) are scalars and equal, this simplifies to: \\[\n(XW - Y)^T (XW - Y) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\nAdding ( W^T W ): \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y + W^T W\n\\]\nCombining like terms: \\[\nL(W) = W^T (X^T X + I) W - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTake the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2(X^T X + I)W - 2X^T Y = 0\n\\]\nSolving for ( W ): \\[\n(X^T X + I)W = X^T Y\n\\] \\[\nW = (X^T X + I)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is: \\[\n\\boxed{W = (X^T X + I)^{-1} X^T Y}\n\\]\nThis result includes a regularization term ( W^T W ), which is common in Ridge Regression or Tikhonov Regularization.\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    N,M = x_poly.shape\n\n    w = (torch.linalg.inv(x_poly.T@x_poly + torch.eye(M))@x_poly.T) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "posts/visualizing-vision-transformers-timm.html",
    "href": "posts/visualizing-vision-transformers-timm.html",
    "title": "Visualize Salincy Maps in Timm Models (ViT) using PyTorch Hooks?",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nfrom skimage import io\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\n\nimg_path = \"/kaggle/input/image-net-visualization/bird_1.png\"\nmodel_path = \"vit_small_patch16_224.dino\"\nimage_size = 1024\n\n\ndef get_image(im_path, shape=image_size):\n    \n    \n    img = io.imread(im_path)[:,:,:3] ## HXWXC\n    H,W,C = img.shape\n    org_img = img.copy()\n    \n    img = A.Normalize()(image=img)[\"image\"] ## HxWxC\n    norm_img = img.copy()\n    \n    img = np.transpose(img,(2,0,1)) ## CxHxW\n    img = np.expand_dims(img,0) ## B=1,CxHxW\n    \n    img = torch.tensor(img)\n    img = nn.Upsample((shape,shape),mode='bilinear')(img)\n    \n    return img, norm_img, org_img\n\n\nimg,norm_img,org_img = get_image(img_path)\n\n\nio.imshow(org_img)\n\n\n\n\n\n\n\n\n\nio.imshow(norm_img)\n\n\n\n\n\n\n\n\n\n%%capture\n! pip install timm\n\n\nimport timm\nmodel = timm.create_model(model_path,pretrained=True,\n                         img_size=image_size,\n                         dynamic_img_size=True)\n\n\n\n\n\nmodel = model.cuda()\n\n\noutputs = {}\ndef get_outputs(name:str):\n    def hook(model, input, output):\n        outputs[name] = output.detach()\n        \n    return hook\n\n\nmodel.blocks[-1].attn.q_norm.register_forward_hook(get_outputs('Q'))\nmodel.blocks[-1].attn.k_norm.register_forward_hook(get_outputs('K'))\nmodel.blocks[-1].register_forward_hook(get_outputs('features'))\n\nmodel(img.cuda())\n\ntensor([[-1.6873e+00,  3.3118e+00,  1.6206e-02,  7.5276e+00, -3.8783e+00,\n          7.9405e-01, -2.5275e-01, -9.2937e-01, -7.6219e+00,  1.8461e+00,\n         -3.7896e+00,  6.9062e+00,  4.9580e+00,  7.0582e+00, -1.4598e+00,\n          1.8604e+00,  3.1002e+00, -2.3646e-01,  9.7898e-01,  1.4716e-01,\n          4.8010e+00,  9.8696e+00, -6.2256e+00, -4.0314e+00,  4.8572e-01,\n          4.6386e+00,  5.2622e+00,  8.2102e+00,  6.6765e+00, -1.0041e+01,\n          9.1688e-01,  2.6036e+00,  1.0652e+00, -1.0549e+01, -1.5771e+00,\n         -2.4766e+00, -4.6636e+00,  5.6768e+00, -1.7559e+00, -4.9503e-01,\n         -2.5628e+00,  1.0277e+01, -4.6592e+00, -3.6269e+00,  6.4073e+00,\n         -2.8656e+00, -8.2731e+00,  9.1952e+00, -2.4074e+00, -5.6380e-01,\n         -3.4731e+00, -4.7151e+00,  1.8822e-01, -3.8197e+00,  2.5455e+00,\n         -1.3325e-01, -4.3335e+00,  7.5765e+00, -3.5712e+00, -4.4365e+00,\n          4.9343e-01,  9.1502e-01,  1.0584e+01,  3.9611e+00,  3.8866e+00,\n         -4.6030e+00,  1.4714e+00,  4.2984e-01, -3.4557e+00,  1.0801e+00,\n         -4.1478e+00,  5.2226e+00,  2.8259e+00,  9.7525e-01,  6.9041e+00,\n          1.2095e-01,  2.4013e+00,  1.6391e+00, -6.0108e+00, -9.3346e+00,\n         -4.0121e+00,  1.8157e+00, -7.2864e-01,  8.5839e+00, -1.0082e+01,\n          1.2923e+00, -1.8654e+00, -1.0347e+01,  4.6857e+00, -2.7471e+00,\n          3.4723e+00,  4.6032e+00, -2.5349e+00, -2.0960e+00,  1.6807e-01,\n          1.9170e+00, -6.1443e+00,  1.0321e+00, -1.6014e+00,  5.8918e+00,\n          9.2721e+00, -1.2786e+00, -6.3613e+00,  5.5546e+00, -1.2483e+01,\n          3.0828e+00, -6.0050e-02, -1.3473e+01,  6.6318e-01, -5.3011e+00,\n         -3.5312e+00, -1.5129e+00,  4.1372e+00,  1.4356e+00,  1.0351e+01,\n         -1.7940e+00, -1.2211e+01,  2.8142e+00, -1.5071e+00, -6.9505e-01,\n         -3.1786e+00,  7.8665e+00,  6.7557e-01,  2.6826e+00, -2.4210e+00,\n         -2.8962e+00,  5.5639e+00, -1.8775e+00, -1.2859e+01, -7.5849e-01,\n          1.6650e+00,  2.6447e+00, -6.3936e-01,  1.5060e+00,  6.0126e-01,\n          3.9601e+00, -1.8724e+00, -3.1135e+00, -2.8265e+00,  1.6418e+00,\n         -5.6146e+00, -9.4629e-02, -1.3282e-01,  7.6737e+00, -1.5090e+00,\n          5.6526e+00, -9.7128e-01, -3.6167e+00,  1.7872e+00, -2.7470e+00,\n         -2.4107e+00, -7.7301e-03,  5.4260e+00, -2.6830e-01, -6.0379e-01,\n          2.4405e+00, -3.0422e+00, -6.1176e+00, -1.3285e+00, -1.1136e+01,\n         -3.1303e+00,  2.8008e+00,  2.7368e+00, -2.8170e+00, -4.1028e+00,\n          5.4076e+00,  1.6082e+00, -9.9726e-01,  5.4990e+00,  1.8774e+00,\n         -5.4387e-01, -2.9433e-01,  3.6973e+00, -4.5119e+00, -3.5505e+00,\n          2.8779e+00,  3.0147e-01, -2.3486e+00, -1.5237e+00,  1.1389e+00,\n          2.2250e+00,  6.8706e+00,  3.7546e+00, -1.6722e+00, -6.4871e-01,\n          3.2394e+00,  6.6144e-01,  1.0893e+00,  1.1054e+00,  4.0759e+00,\n         -4.9504e+00, -4.0154e+00, -1.4490e+00,  1.8689e+00, -3.9057e+00,\n          2.2533e+00, -6.3996e+00, -5.5835e-01,  2.4417e+00,  2.1308e+00,\n          3.9925e+00,  4.4550e-01,  1.1531e+00,  4.7416e+00,  2.0096e+00,\n         -1.2605e+00, -9.5855e-01, -5.2844e+00, -4.0194e+00,  7.0700e-01,\n          5.4294e+00,  3.3919e+00,  4.9089e+00,  4.1371e-01,  2.2920e+00,\n         -1.7367e-01, -4.7534e+00,  5.0581e+00,  8.1151e+00, -1.5215e+00,\n          3.7808e+00, -2.9530e+00, -9.3015e+00,  6.1947e-01, -3.9256e-01,\n         -4.1782e+00,  1.9628e+00,  6.7975e+00,  1.2619e+00,  1.7882e+00,\n         -9.8904e-01, -3.4642e-01, -2.1700e+00,  8.9316e+00,  4.0332e+00,\n         -3.3242e+00,  3.7758e+00,  2.3272e+00, -2.6639e+00,  1.8211e+00,\n          2.0843e+00,  9.2511e+00,  7.0950e-01,  2.3567e+00, -3.4504e+00,\n         -1.2123e+01, -2.2370e-01, -4.0639e+00, -7.6606e-01, -5.7467e-01,\n          6.2722e+00, -1.1846e+00,  7.4569e-01, -2.3022e+00, -7.6787e+00,\n          1.5879e+00, -5.4248e+00,  4.4352e+00, -3.0992e-01, -9.8239e+00,\n          4.7596e+00, -7.8933e+00, -1.5991e+00,  3.5556e+00, -8.2829e+00,\n         -2.8837e+00,  8.0843e+00,  4.4499e+00, -7.8938e+00,  1.1607e+01,\n          3.3790e+00, -1.0773e+01,  4.5276e+00,  2.0844e+00,  2.4876e+00,\n          2.4884e+00,  2.1223e-01, -3.0398e+00, -6.3939e+00, -2.2402e+00,\n         -9.1261e-01,  2.2716e+00,  7.5233e+00,  3.2600e+00,  1.3579e+00,\n          3.0965e+00, -1.2645e+00,  3.8311e-01, -3.8111e+00, -1.9618e+00,\n          2.7574e+00,  1.0933e+01, -8.1753e+00, -5.7976e-01,  4.3519e+00,\n         -3.1721e+00,  3.5473e+00,  8.9204e-01, -5.8862e+00,  5.9441e+00,\n         -4.1584e+00,  2.6372e+00,  5.9613e-01, -3.5491e+00, -7.7043e+00,\n         -5.9683e+00, -1.9161e-01, -3.4110e+00, -4.9706e+00,  3.6422e+00,\n          2.0380e+00, -1.7211e-01,  2.2398e+00, -8.1027e-01, -1.7448e+00,\n         -8.5672e-01, -3.8151e-02, -1.5993e+00,  3.6409e+00,  3.0737e+00,\n          1.5809e-01, -2.5176e-01,  3.4933e+00, -6.0633e+00,  6.0766e+00,\n          6.8786e+00, -2.1457e+00,  4.7027e-01, -5.3296e-01, -9.8374e+00,\n         -4.7503e+00, -1.6798e+01,  5.6179e+00,  1.0797e+00,  9.5722e-01,\n          1.0259e+01,  2.4940e+00,  5.1237e+00, -3.9690e+00,  6.3755e+00,\n         -2.4782e+00,  5.0692e+00,  1.1048e+00, -3.4466e+00, -8.6174e+00,\n         -1.3207e+00, -1.0411e+01, -1.8757e+00, -1.1057e+01, -5.9575e+00,\n          1.0189e+00,  2.0985e+00, -3.2184e+00, -3.6003e+00, -4.3049e+00,\n          3.8259e+00, -4.0951e+00, -1.8868e+00, -4.7576e+00, -6.7396e+00,\n          6.9816e-01, -2.2439e+00, -4.5493e+00,  8.9310e+00, -1.6068e-01,\n         -2.5645e+00,  7.0637e+00,  1.7403e+01,  2.9385e+00,  1.5559e+00,\n          5.3462e+00,  1.3728e+01,  3.9456e+00, -5.9510e+00,  3.1551e+00,\n          4.6709e+00, -3.6971e+00, -1.2921e+00,  2.9455e+00,  4.7313e+00,\n         -2.5174e+00,  6.0789e+00, -7.3726e+00, -2.1013e+00]], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nscale = model.blocks[-1].attn.scale\n\n\noutputs[\"attention\"] = (outputs[\"Q\"]@ outputs[\"K\"].transpose(-2, -1))#*scale\n# outputs[\"attention\"] = outputs[\"attention\"].softmax(dim=-1)\n\n\noutputs[\"attention\"].shape\n\ntorch.Size([1, 6, 4097, 4097])\n\n\n\nb,num_heads,num_patches_1,num_patches_1 = outputs[\"attention\"].shape\nmap_size = int(np.sqrt(num_patches_1-1))\nfor attention_head in range(num_heads):\n    attention_map = outputs[\"attention\"][:,attention_head,0,1:] ## 1,4096\n    attention_map = attention_map.view(1,1,map_size,map_size)\n    \n    attention_map = nn.Upsample(size=(image_size,image_size))(attention_map)\n    \n    attention_map = attention_map[0,0,:,:].detach().cpu().numpy()\n    \n    io.imshow(attention_map)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutputs[\"features\"].shape\n\ntorch.Size([1, 4097, 384])\n\n\n\nfeatures = outputs[\"features\"].mean(-1)\nfeatures = features[:,1:]\n\nfeatures = features.view(1,1,map_size,map_size)\n\nfeatures = nn.Upsample(size=(image_size,image_size))(features)\n\nfeatures = features[0,0,:,:].detach().cpu().numpy()\n\nio.imshow(features)\n\n/opt/conda/lib/python3.10/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Bibhabasu’s Blog Page",
    "section": "",
    "text": "Hi there 👋 This is my Blog Page - Though I wanted to create my personal site using Nextjs or React only, but some how I love blogging on quarto . . . now using sayak’s already available (which was used for my previous site, got back to blog on quarto) my site - bibhabasu-mohapatra.space\nI am Bibhabasu Mohapatra. Currently working as a Data Scientist at Microsoft, previously working as an Engineer(Computer Vision) at Aira Matrix.\nI’m particularly interested and want to work in topics related to self-supervised learning, multi-modal learning, and Representations \nOpen for Research Opportunities in Academia. for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression from Scratch\n\n\n\n\n\nLinear Regression from PRML Example 1 from Chapter 1 Introdcution\n\n\n\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize Salincy Maps in Timm Models (ViT) using PyTorch Hooks?\n\n\n\n\n\nTutorial code of how I Visualize Attention Maps and Feature Maps (Salieancy Maps) in PyTorch timm using PyTorch Hooks as timm does not provide attention intermediates\n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding how EMA works?\n\n\n\n\n\nUnderstanding how EMA works with an simulation of a toy model with toy weights\n\n\n\n\n\nAug 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating Deep Supervision in Segmentation Models Pytorch (smp)?\n\n\n\n\n\nTutorial code of how I integrated Deep Supervision Loss generation setup with Segmentation Models PyTorch Framework, which could be Directly us with Minimal changes\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bibhabasu’s Blog Page",
    "section": "",
    "text": "Hi there 👋 This is my Blog Page - Though I wanted to create my personal site using Nextjs or React only, but some how I love blogging on quarto . . . now using sayak’s already available (which was used for my previous site, got back to blog on quarto) my site - bibhabasu-mohapatra.space\nI am Bibhabasu Mohapatra. Currently working as a Data Scientist at Microsoft, previously working as an Engineer(Computer Vision) at Aira Matrix.\nI’m particularly interested and want to work in topics related to self-supervised learning, multi-modal learning, and Representations \nOpen for Research Opportunities in Academia. for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression from Scratch\n\n\n\n\n\nLinear Regression from PRML Example 1 from Chapter 1 Introdcution\n\n\n\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize Salincy Maps in Timm Models (ViT) using PyTorch Hooks?\n\n\n\n\n\nTutorial code of how I Visualize Attention Maps and Feature Maps (Salieancy Maps) in PyTorch timm using PyTorch Hooks as timm does not provide attention intermediates\n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding how EMA works?\n\n\n\n\n\nUnderstanding how EMA works with an simulation of a toy model with toy weights\n\n\n\n\n\nAug 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating Deep Supervision in Segmentation Models Pytorch (smp)?\n\n\n\n\n\nTutorial code of how I integrated Deep Supervision Loss generation setup with Segmentation Models PyTorch Framework, which could be Directly us with Minimal changes\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "",
    "text": "image.png\n\n\n::: {#cell-2 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-03-31T15:18:34.645310Z”,“iopub.status.busy”:“2023-03-31T15:18:34.645013Z”,“iopub.status.idle”:“2023-03-31T15:19:08.473332Z”,“shell.execute_reply”:“2023-03-31T15:19:08.471964Z”,“shell.execute_reply.started”:“2023-03-31T15:18:34.645281Z”}’ trusted=‘true’ execution_count=1}\n%%capture\n!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n:::\n\n\n\n\n\nexample :\n\ntorch.Size([1, 1, 384, 384])\ntorch.Size([1, 1, 192, 192])\ntorch.Size([1, 1, 96, 96])\ntorch.Size([1, 1, 48, 48])\ntorch.Size([1, 1, 24, 24])\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom segmentation_models_pytorch.base import modules as md\nfrom segmentation_models_pytorch.encoders import get_encoder\nfrom segmentation_models_pytorch.base import (\n    SegmentationModel,\n    SegmentationHead,\n    \n)\n\nimport segmentation_models_pytorch as smp\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        use_batchnorm=True,\n        attention_type=None,\n    ):\n        super().__init__()\n        self.conv1 = md.Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n        self.conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(nn.Sequential):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        conv1 = md.Conv2dReLU(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        super().__init__(conv1, conv2)\n\n\nclass UnetDecoder(nn.Module):\n    def __init__(\n        self,\n        encoder_channels,\n        decoder_channels,\n        n_blocks=5,\n        use_batchnorm=True,\n        attention_type=None,\n        center=False,\n    ):\n        super().__init__()\n\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[1:]\n        # reverse channels to start from head of encoder\n        encoder_channels = encoder_channels[::-1]\n\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        skip_channels = list(encoder_channels[1:]) + [0]\n        out_channels = decoder_channels\n\n        if center:\n            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n        else:\n            self.center = nn.Identity()\n\n        # combine decoder keyword arguments\n        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n        blocks = [\n            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, *features):\n\n        features = features[1:]  # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        head = features[0]\n        skips = features[1:]\n\n        x = self.center(head)\n\n        out = []\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i &lt; len(skips) else None\n            x = decoder_block(x, skip)\n            out.append(x)\n\n        return out[::-1]\n    \n\nclass SegmentationModel(nn.Module):\n    def __init__(self, \n                encoder,\n                encoder_weights=None,\n                encoder_depth=5,\n                in_channels=3,\n                decoder_use_batchnorm: bool = True,\n                decoder_channels = (256, 128, 64, 32, 16),\n                decoder_attention_type = None,\n                classes = 1\n                 ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = UnetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            center=False,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = nn.ModuleList()\n        for channel in decoder_channels[::-1]:\n            self.segmentation_head.append(\n                SegmentationHead(\n                    in_channels=channel,\n                    out_channels=classes,\n                    activation=None,\n                    kernel_size=3,\n                )\n            )\n\n    def forward(self,x):\n\n        features = self.encoder(x)\n        decoder_output = self.decoder(*features)\n        \n        masks = []\n        for i,seg_head in enumerate(self.segmentation_head):\n            masks.append(seg_head(decoder_output[i]))\n\n        return masks\n\n\n\n\nin_ = torch.zeros((1,3,384,384))\n\nmodel = SegmentationModel(\"resnet18\")\nmodel.to(\"cuda\")\n\nprint(\"The following are the expected outputs\")\nfor i in model(in_.to(\"cuda\")):\n    print(i.shape)\n\nThe following are the expected outputs\ntorch.Size([1, 1, 384, 384])\ntorch.Size([1, 1, 192, 192])\ntorch.Size([1, 1, 96, 96])\ntorch.Size([1, 1, 48, 48])\ntorch.Size([1, 1, 24, 24])\n\n\n\n\n\n\n\n\n\nclass Deep_Supervised_Loss(nn.Module):\n    def __init__(self):\n\n        super().__init__()\n        self.loss = smp.losses.TverskyLoss(mode=\"binary\",from_logits=False,)\n\n    def forward(self, input, target):\n        loss = 0\n        # print(type(input))\n        for i, img in enumerate(input):\n            w = 1 / (2 ** i)\n            \n            label = F.interpolate(target,size=img.shape[2:])\n\n            l = self.loss(torch.sigmoid(img), label)\n            \n            loss += l * w\n            \n        return loss     \n\n\n\n\n\n\nENCODER = \"resnet18\"\nENCODER_WEIGHTS = \"imagenet\"\n\n\n\n\nmodel = SegmentationModel(encoder=ENCODER,encoder_weights=ENCODER_WEIGHTS)\n\n\ntrain_image = torch.randn((36,3,256,256),device=\"cuda\")\ntrain_labels = torch.ones((36,1,256,256),device=\"cuda\")\n\n\nmodel.to(\"cuda\")\nprediction = model(train_image)\n\n\n\n\n\n\nprint(\"look at the shape of Predictions\")\nfor out in prediction:\n    print(out.shape)\n\nlook at the shape of Predictions\ntorch.Size([36, 1, 256, 256])\ntorch.Size([36, 1, 128, 128])\ntorch.Size([36, 1, 64, 64])\ntorch.Size([36, 1, 32, 32])\ntorch.Size([36, 1, 16, 16])\n\n\n\n\n\n\nloss = Deep_Supervised_Loss()(prediction,train_labels)\nprint(loss.item())\n\n0.6200616359710693"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#here-directly-copied-the-decoder-code-of-segmentation-model-pytorch-unet.-and-in-the-class-unetdecoder-i-simply-also-appended-the-outputs-in-the-decoder-and-passed-it-also-through-different-segmentation-heads-according-to-their-channels.",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#here-directly-copied-the-decoder-code-of-segmentation-model-pytorch-unet.-and-in-the-class-unetdecoder-i-simply-also-appended-the-outputs-in-the-decoder-and-passed-it-also-through-different-segmentation-heads-according-to-their-channels.",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "",
    "text": "example :\n\ntorch.Size([1, 1, 384, 384])\ntorch.Size([1, 1, 192, 192])\ntorch.Size([1, 1, 96, 96])\ntorch.Size([1, 1, 48, 48])\ntorch.Size([1, 1, 24, 24])\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom segmentation_models_pytorch.base import modules as md\nfrom segmentation_models_pytorch.encoders import get_encoder\nfrom segmentation_models_pytorch.base import (\n    SegmentationModel,\n    SegmentationHead,\n    \n)\n\nimport segmentation_models_pytorch as smp\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        use_batchnorm=True,\n        attention_type=None,\n    ):\n        super().__init__()\n        self.conv1 = md.Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n        self.conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(nn.Sequential):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        conv1 = md.Conv2dReLU(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        super().__init__(conv1, conv2)\n\n\nclass UnetDecoder(nn.Module):\n    def __init__(\n        self,\n        encoder_channels,\n        decoder_channels,\n        n_blocks=5,\n        use_batchnorm=True,\n        attention_type=None,\n        center=False,\n    ):\n        super().__init__()\n\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[1:]\n        # reverse channels to start from head of encoder\n        encoder_channels = encoder_channels[::-1]\n\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        skip_channels = list(encoder_channels[1:]) + [0]\n        out_channels = decoder_channels\n\n        if center:\n            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n        else:\n            self.center = nn.Identity()\n\n        # combine decoder keyword arguments\n        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n        blocks = [\n            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, *features):\n\n        features = features[1:]  # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        head = features[0]\n        skips = features[1:]\n\n        x = self.center(head)\n\n        out = []\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i &lt; len(skips) else None\n            x = decoder_block(x, skip)\n            out.append(x)\n\n        return out[::-1]\n    \n\nclass SegmentationModel(nn.Module):\n    def __init__(self, \n                encoder,\n                encoder_weights=None,\n                encoder_depth=5,\n                in_channels=3,\n                decoder_use_batchnorm: bool = True,\n                decoder_channels = (256, 128, 64, 32, 16),\n                decoder_attention_type = None,\n                classes = 1\n                 ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = UnetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            center=False,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = nn.ModuleList()\n        for channel in decoder_channels[::-1]:\n            self.segmentation_head.append(\n                SegmentationHead(\n                    in_channels=channel,\n                    out_channels=classes,\n                    activation=None,\n                    kernel_size=3,\n                )\n            )\n\n    def forward(self,x):\n\n        features = self.encoder(x)\n        decoder_output = self.decoder(*features)\n        \n        masks = []\n        for i,seg_head in enumerate(self.segmentation_head):\n            masks.append(seg_head(decoder_output[i]))\n\n        return masks\n\n\n\n\nin_ = torch.zeros((1,3,384,384))\n\nmodel = SegmentationModel(\"resnet18\")\nmodel.to(\"cuda\")\n\nprint(\"The following are the expected outputs\")\nfor i in model(in_.to(\"cuda\")):\n    print(i.shape)\n\nThe following are the expected outputs\ntorch.Size([1, 1, 384, 384])\ntorch.Size([1, 1, 192, 192])\ntorch.Size([1, 1, 96, 96])\ntorch.Size([1, 1, 48, 48])\ntorch.Size([1, 1, 24, 24])"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#this-is-my-custom-loss-function-that-takes-output-from-all-decoder-block-and-interpolates-the-ground-truthlabelstargets-and-calculates-the-loss.",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#this-is-my-custom-loss-function-that-takes-output-from-all-decoder-block-and-interpolates-the-ground-truthlabelstargets-and-calculates-the-loss.",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "",
    "text": "class Deep_Supervised_Loss(nn.Module):\n    def __init__(self):\n\n        super().__init__()\n        self.loss = smp.losses.TverskyLoss(mode=\"binary\",from_logits=False,)\n\n    def forward(self, input, target):\n        loss = 0\n        # print(type(input))\n        for i, img in enumerate(input):\n            w = 1 / (2 ** i)\n            \n            label = F.interpolate(target,size=img.shape[2:])\n\n            l = self.loss(torch.sigmoid(img), label)\n            \n            loss += l * w\n            \n        return loss"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#good-news-is-all-smps-and-timms-pretrained-weight-work-here",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#good-news-is-all-smps-and-timms-pretrained-weight-work-here",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "",
    "text": "ENCODER = \"resnet18\"\nENCODER_WEIGHTS = \"imagenet\"\n\n\n\n\nmodel = SegmentationModel(encoder=ENCODER,encoder_weights=ENCODER_WEIGHTS)\n\n\ntrain_image = torch.randn((36,3,256,256),device=\"cuda\")\ntrain_labels = torch.ones((36,1,256,256),device=\"cuda\")\n\n\nmodel.to(\"cuda\")\nprediction = model(train_image)"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#output-is-a-list-whose-length-is-same-as-encoder-length-0th-index-is-the-main-index-of-prediction",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#output-is-a-list-whose-length-is-same-as-encoder-length-0th-index-is-the-main-index-of-prediction",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "",
    "text": "print(\"look at the shape of Predictions\")\nfor out in prediction:\n    print(out.shape)\n\nlook at the shape of Predictions\ntorch.Size([36, 1, 256, 256])\ntorch.Size([36, 1, 128, 128])\ntorch.Size([36, 1, 64, 64])\ntorch.Size([36, 1, 32, 32])\ntorch.Size([36, 1, 16, 16])"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#implementation-output-of-loss-function",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#implementation-output-of-loss-function",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "",
    "text": "loss = Deep_Supervised_Loss()(prediction,train_labels)\nprint(loss.item())\n\n0.6200616359710693"
  },
  {
    "objectID": "posts/understanding-how-ema-works.html",
    "href": "posts/understanding-how-ema-works.html",
    "title": "Understanding how EMA works?",
    "section": "",
    "text": "EMA (Exponential Moving Average) is an incredibly useful concept that finds application in various scenarios:\n\nWeight Updates: EMA is used for updating model weights while retaining a historical record of previous weights. This enables the model to blend new information with past knowledge effectively.\nSelf-Supervised Learning: EMA is commonly employed in Self-Supervised Learning setups. The weights obtained from Self-Supervised Learning are often utilized for downstream tasks like classification and segmentation.\n\n\n\nInitially, there was a misconception that the EMA process directly impacts the ongoing training of model weights. However, this is not the case. In reality, the EMA process involves the creation of a duplicated set of weights. These duplicate weights are updated alongside the primary training process, and the updated weights are subsequently leveraged for validation purposes. As a result, the overall training procedure remains unaffected by the EMA process.\n\n\n\nimage.png\n\n\n::: {#36edc225 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-08-14T15:14:27.136587Z”,“iopub.status.busy”:“2023-08-14T15:14:27.136194Z”,“iopub.status.idle”:“2023-08-14T15:14:30.874853Z”,“shell.execute_reply”:“2023-08-14T15:14:30.873447Z”}’ papermill=‘{“duration”:3.746175,“end_time”:“2023-08-14T15:14:30.877588”,“exception”:false,“start_time”:“2023-08-14T15:14:27.131413”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport torch.nn as nn\nimport torch\nfrom copy import deepcopy\n\nclass Model(nn.Module):\n    def __init__(self,):\n        super().__init__()\n\n        self.layer = nn.Linear(3,3,bias=False)\n\n    def forward(self,x):\n        return self.layer(x)\n    \n@torch.no_grad()\ndef init_weights_1(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n@torch.no_grad()\ndef init_weights_2(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(2.0)\n        print(m.weight)\n:::\n\nteacher = Model().apply(init_weights_1) ## TO BE UPDATED\nstudent = Model().apply(init_weights_2) ## From Training\n\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\n\n\n\n@torch.no_grad()\ndef ema_model(teacher_model, student_model,decay=0.5):\n    teacher_model.eval()\n\n    for teacher_wt,student_wt in zip(teacher_model.state_dict().values(),student_model.state_dict().values()):\n        teacher_wt.copy_(teacher_wt*decay + student_wt*(1-decay))\n\n\nema_model(teacher,student)\n\n\nteacher.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000]]))])\n\n\n\nstudent.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[2., 2., 2.],\n                      [2., 2., 2.],\n                      [2., 2., 2.]]))])"
  },
  {
    "objectID": "posts/understanding-how-ema-works.html#exponential-moving-average-ema-in-weight-updates",
    "href": "posts/understanding-how-ema-works.html#exponential-moving-average-ema-in-weight-updates",
    "title": "Understanding how EMA works?",
    "section": "",
    "text": "EMA (Exponential Moving Average) is an incredibly useful concept that finds application in various scenarios:\n\nWeight Updates: EMA is used for updating model weights while retaining a historical record of previous weights. This enables the model to blend new information with past knowledge effectively.\nSelf-Supervised Learning: EMA is commonly employed in Self-Supervised Learning setups. The weights obtained from Self-Supervised Learning are often utilized for downstream tasks like classification and segmentation.\n\n\n\nInitially, there was a misconception that the EMA process directly impacts the ongoing training of model weights. However, this is not the case. In reality, the EMA process involves the creation of a duplicated set of weights. These duplicate weights are updated alongside the primary training process, and the updated weights are subsequently leveraged for validation purposes. As a result, the overall training procedure remains unaffected by the EMA process.\n\n\n\nimage.png\n\n\n::: {#36edc225 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-08-14T15:14:27.136587Z”,“iopub.status.busy”:“2023-08-14T15:14:27.136194Z”,“iopub.status.idle”:“2023-08-14T15:14:30.874853Z”,“shell.execute_reply”:“2023-08-14T15:14:30.873447Z”}’ papermill=‘{“duration”:3.746175,“end_time”:“2023-08-14T15:14:30.877588”,“exception”:false,“start_time”:“2023-08-14T15:14:27.131413”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport torch.nn as nn\nimport torch\nfrom copy import deepcopy\n\nclass Model(nn.Module):\n    def __init__(self,):\n        super().__init__()\n\n        self.layer = nn.Linear(3,3,bias=False)\n\n    def forward(self,x):\n        return self.layer(x)\n    \n@torch.no_grad()\ndef init_weights_1(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n@torch.no_grad()\ndef init_weights_2(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(2.0)\n        print(m.weight)\n:::\n\nteacher = Model().apply(init_weights_1) ## TO BE UPDATED\nstudent = Model().apply(init_weights_2) ## From Training\n\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\n\n\n\n@torch.no_grad()\ndef ema_model(teacher_model, student_model,decay=0.5):\n    teacher_model.eval()\n\n    for teacher_wt,student_wt in zip(teacher_model.state_dict().values(),student_model.state_dict().values()):\n        teacher_wt.copy_(teacher_wt*decay + student_wt*(1-decay))\n\n\nema_model(teacher,student)\n\n\nteacher.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000]]))])\n\n\n\nstudent.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[2., 2., 2.],\n                      [2., 2., 2.],\n                      [2., 2., 2.]]))])"
  }
]