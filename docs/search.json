[
  {
    "objectID": "posts/linear-regression-from-scratch.html",
    "href": "posts/linear-regression-from-scratch.html",
    "title": "Polynomial Curve Fitting and doing Linear Regression from Scratch (!! Four ways to Solve a Least Squares Problem)",
    "section": "",
    "text": "image.png\nThis blog is a reading follow up of the book Pattern Recognition in Machine Learning by Chris Bishop. In his first example in the chapter he explores curve fitting. There are lot of equations directly at you, and if you understand it easy and beautiful. I come from more of a sklearn, and pytorch setting the world of Machine learning is kind of black box who just use pytorch nn bindings or sklearn linear regresion or xgboost for work. But the the Introduction to Gilbert strangs lecture or book like PRML will clean your mind and will act as detox for your brain which is filled with hundreds of abstraction and libraries. And the detox from hundreds of libraries will make you understand the TRUE WORLD made by us humans. While implementinhg this I realised that we just nead to find Pseudo-Inverses of Weight Matrix, which would also scale to test data. . . its just not all about fancy SGD optimization rather it can be about just finding Inverses of a Matrix to solve your world problems"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#plot-training-data",
    "href": "posts/linear-regression-from-scratch.html#plot-training-data",
    "title": "Polynomial Curve Fitting and doing Linear Regression from Scratch (!! Four ways to Solve a Least Squares Problem)",
    "section": "plot training data",
    "text": "plot training data\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train)"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#fit-the-linear-model-on-training-data",
    "href": "posts/linear-regression-from-scratch.html#fit-the-linear-model-on-training-data",
    "title": "Polynomial Curve Fitting and doing Linear Regression from Scratch (!! Four ways to Solve a Least Squares Problem)",
    "section": "Fit the Linear model on Training data",
    "text": "Fit the Linear model on Training data\n\nPart below directly comes from Gilbert Strang‚Äôs Lectures at MIT (Lecture 09 - Four Ways to Solve Least Square Problems)[https://youtu.be/ZUU57Q3CFOU?si=WSYz5c10C4AKISjD] believe me world is just filled with Least Square Problems everywhere\nTo solve for the optimal weight matrix ( W ), given the expression ( (XW - Y)^T (XW - Y) ), we can proceed by first expanding the expression and then minimizing it with respect to ( W ).\n\n\nStep 1: Expand the expression\nGiven: \\[\nL(W) = (XW - Y)^T (XW - Y)\n\\]\nExpanding this: \\[\nL(W) = (XW)^T(XW) - (XW)^T Y - Y^T (XW) + Y^T Y\n\\]\nSimplifying further: \\[\nL(W) = W^T X^T XW - W^T X^T Y - Y^T XW + Y^T Y\n\\]\nSince ( W^T X^T Y ) and ( Y^T XW ) are scalars, they are equal. Therefore: \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\n\n\nStep 2: Minimize the expression with respect to ( W )\nTo find the optimal ( W ), take the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2X^T XW - 2X^T Y = 0\n\\]\nSimplifying: \\[\nX^T XW = X^T Y\n\\]\nFinally, solve for ( W ): \\[\nW = (X^T X)^{-1} X^T Y\n\\]\n\n\nFinal Answer\nThe optimal weight matrix ( W ) is given by: \\[\n\\boxed{W = (X^T X)^{-1} X^T Y}\n\\]\nThis result is commonly known as the Normal Equation in linear regression.\n\npowers = torch.arange(5).float()\nx_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\nw = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n\n\nw.shape, x_test.shape\n\n(torch.Size([5, 1]), torch.Size([200]))\n\n\n\nx_poly_test = x_test.unsqueeze(-1).pow(powers)\ny_pred = x_poly_test @ w\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\nplt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    w = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#example-polynomial-curve-fitting-with-regularization",
    "href": "posts/linear-regression-from-scratch.html#example-polynomial-curve-fitting-with-regularization",
    "title": "Polynomial Curve Fitting and doing Linear Regression from Scratch (!! Four ways to Solve a Least Squares Problem)",
    "section": "1.1. Example: Polynomial Curve Fitting with Regularization",
    "text": "1.1. Example: Polynomial Curve Fitting with Regularization\nOne technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified error function of the form.\nTechniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefficients. The particular case of a quadratic regularizer is called ridge regres\u0002sion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is known as weight decay\nTo solve for the optimal weight matrix ( W ) given the objective function ( (XW - Y)^T (XW - Y) + W^T W ), follow these steps:\n\nObjective Function\nThe objective function is: \\[\nL(W) = (XW - Y)^T (XW - Y) + W^T W\n\\]\n\n\nStep 1: Expand the Expression\nExpanding ( (XW - Y)^T (XW - Y) ): \\[\n(XW - Y)^T (XW - Y) = (XW)^T XW - (XW)^T Y - Y^T XW + Y^T Y\n\\]\nSince ( (XW)^T Y ) and ( Y^T XW ) are scalars and equal, this simplifies to: \\[\n(XW - Y)^T (XW - Y) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\nAdding ( W^T W ): \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y + W^T W\n\\]\nCombining like terms: \\[\nL(W) = W^T (X^T X + I) W - 2W^T X^T Y + Y^T Y\n\\]\n\n\nStep 2: Minimize the Expression with Respect to ( W )\nTake the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2(X^T X + I)W - 2X^T Y = 0\n\\]\nSolving for ( W ): \\[\n(X^T X + I)W = X^T Y\n\\] \\[\nW = (X^T X + I)^{-1} X^T Y\n\\]\n\n\nFinal Answer\nThe optimal weight matrix ( W ) is: \\[\n\\boxed{W = (X^T X + I)^{-1} X^T Y}\n\\]\nThis result includes a regularization term ( W^T W ), which is common in Ridge Regression or Tikhonov Regularization.\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    N,M = x_poly.shape\n\n    w = (torch.linalg.inv(x_poly.T@x_poly + torch.eye(M))@x_poly.T) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "posts/thresholding-using-scikit-image.html",
    "href": "posts/thresholding-using-scikit-image.html",
    "title": "Thresholding using scikit image (part of my kaggle submission code)",
    "section": "",
    "text": "When I attempted segmentation problems and generated masks I faced a significant problem with thresholding parameters. If you have a mask or image with a very sparse pixel value all over then you won‚Äôt be able to see the real image rather it acts like a secret note which is only visible when you show it in sunlight.I searched for a way in my Kaggle Competition submission to convert a mask to a binary mask without randomly setting the threshold. lINK TO THE 80TH Place Notebook Here - https://www.kaggle.com/code/nishantbhansali/rank-80th-train-coat-hubmap-final-for-lb-part-3\nmy mask looked like this:\n\n\n\nimage.png\n\n\nI was randomly trying out thresholds like 0.5, 0.70, . . . 0.14 . . . and it is almost impossible to do the same for every experiment. Rather I searched for a how-to convert masks to binary masks with an automated threshold. Then I stumbled upon thresholding on the scikit image and the tutorial is very simple to use any thresholding. Link &gt;&gt; from skimage.filters import threshold_minimum, threshold_isodata, threshold_li, threshold_mean, threshold_otsu\nI tried all of them for my different experiments. And each of them had a different result. My final submission looked like this, which can smoothen a bit more this gave me a top leaderboard accuracy like a 0.78 Dice Score on LeaderBoard here.\nMore on different types of thresholding techniques :\nThe thresholding technique‚Äôs basic idea is ‚Äî\n\nto find a threshold by performing some operation on the Image i.e.¬†threshold = Function(Image)\nBinary_Image = Image &gt; threshold\n\n\nmask = transform.resize(mask, (df_valid.img_height, df_valid.img_width))\n\nthreshold = filters.threshold_mean(mask) #threshold_isodata, threshold_otsu, threshold_li, threshold_mean etc.\nmask = mask &gt; threshold\nmask = mask.astype(np.int8)\n\nSo it is like when we do mean thresholding it is like threshold = Mean(Image). Where in minimum thresholding, thresholding = Minimum(Image).\nBut the techniques like Otsu‚Äôs Threshold go beyond the ways of Minimum or mean to find the Threshold."
  },
  {
    "objectID": "posts/language is-not-a-formal-system-language-is-glorious-chaos.html",
    "href": "posts/language is-not-a-formal-system-language-is-glorious-chaos.html",
    "title": "Language is Not a formal System, Language is glorious Chaos - Chris Manning (cs224n)",
    "section": "",
    "text": "‚ÄúLanguage is Not a formal System, Language is glorious Chaos‚Äù ‚Äî . . . Chris Manning , CS224N ( https://xkcd.com/1576/)\n\n\n\nimage.png\n\n\n\nMeaning of a Word\nThe idea that is represented by words, phrases, etc. is the idea that a person wants to convey using words, signs, etc. This idea is expressed in a work of writing, art, etc.\n\n\nRepresenting Words as Discrete Symbols\nIn NLP, we treat words as discrete symbols. For example, consider the words ‚Äúhotel,‚Äù ‚Äúconference,‚Äù and ‚Äúmotel‚Äù as Localist Representations using one-hot vectors:\n\nmotel = [0, 0, ‚Ä¶, 0, 1, 0, 0, 0]\nhotel = [0, 0, 1, 0, ‚Ä¶, 0, 0, 0]\n\nWhat will be the dimension of the vector?\nThe dimension of the vector will be the number of words in the vocabulary. If a language has only two words, symbols, or signs in its vocabulary, then it will have two vectors: [0,1] and [1,0].\n\n\nProblems with Words as Discrete Symbols\nIt‚Äôs very important to understand and address the issues with treating words as discrete symbols. For instance, consider the vectors for ‚Äúmotel‚Äù and ‚Äúhotel.‚Äù Aren‚Äôt they orthogonal vectors?\nWhat are orthogonal vectors?\nOrthogonal vectors are vectors that are perpendicular to each other, meaning their dot product is zero. Let‚Äôs calculate the dot product of ‚Äúmotel‚Äù and ‚Äúhotel‚Äù ‚Äî it‚Äôs zero!\n\n\nWhat if the Dot Product is Zero?\nConsider two 2-D vectors A and B. We can represent A and B as:\n\nA = 2i + 3j\nB = 2i + 2j\n\n(where i and j are unit vectors along the x and y axes)\nWe can then ask how much A lies on B. The answer will be the component of A along B. So, what is this component?"
  },
  {
    "objectID": "posts/Start-with-Transformers-for-Beginners.html",
    "href": "posts/Start-with-Transformers-for-Beginners.html",
    "title": "Start with Transformers for Beginners",
    "section": "",
    "text": "copying to my new blog page from :https://medium.com/@bibhabasumohapatrabm/start-with-transformers-for-beginners-2-5040c2acab7b\nLink to the previous blog ‚Äî https://medium.com/@bibhabasumohapatrabm/start-with-transformers-for-beginners-54e147ca5a7c\nAttention Mechanism fails as the RNN computations are inherently sequential and not parallel. In contrast, that is not the case with Transformers, So a new concept is introduced, instead of relying on the recurrence, we use special attention called self-attention.\n‚ÄúAttention in RNNs refers to the computing relevance of each encoder‚Äôs hidden state to the decoder‚Äôs hidden state. In RNNs we use the fixed embedding for each token.\nWhere ‚Äúself‚Äù refers to the fact that these weights are computed for all hidden states, we use the sequence to compute the weighted average of each embedding.‚Äù ‚Äî NLP with Transformers.\nSummarizing in simple words:\nSo basically, in ATTENTION it‚Äôs like, we have to encode a vector of fixed length and wait for the corresponding output vector and repeat the process and at the same time input the outputs of the corresponding inputs and give that to the other time-stamped inputs to use attention.\nWhere ‚Äú SELF ‚Äú ATTENTION helps us to get outputs and attention simultaneously, weights are different for each input embedding -\n\\[\\bar{x}_i = \\sum w_{ij} x_i\\]\nQ ‚Üí What are contextualized embeddings?\nExample: ‚ÄùTime flies like an arrow‚Äù ‚Üí flies here is referred to as a verb.\nrather than insects.\ncontextual embeddings:\nWe use the representation of the word, that fits the context i.e.¬†the words neighboring the word ‚Äúflies‚Äù, ‚ÄúTime‚Äù and ‚ÄúArrow‚Äù tells us that ‚Äúflies‚Äù is Verb. Hence, providing more weight Wij to the embeddings of ‚ÄúTime‚Äù and ‚ÄúArrow‚Äù will be a better choice, and embeddings generated this way are called Contextualized Embeddings.\n\n\n\nimage.png\n\n\nIn the Paper ‚ÄúAttention is all you need‚Äù\nWe were introduced to the Scaled- dot product Attention:\nScaled-dot Attention is one of the ways to implement self-attention layers -\nwe have three vectors query, key, and value. a good example in the Book NLP with transformers makes it easy to understand what is a query, key, and value. When we go to the Supermarket, we have a dish recipe and the ingredients in the recipe are queries. ‚Äî As you scan through shelves, you will check every label. So labels are key. ‚Äî When you have while scanning, you take that item. So the item is the value. 2. We calculate how much query and key relate/are similar to each other, we do this by dot product, for more about similarity and dot-product ‚Üí go to this blog by me. we gt Q.K (dot product) ‚Üí these dot products are called attention scores, So if there are N inputs then there is an NxN matrix of Attention scores\n\nTo normalize large values, attention scores are multiplied by a scaling factor to normalize and then normalized with a softmax to ensure all the column values sum to 1. We get, attention weights Wij\n\n\\[w_{ij} = \\text{Normalized}(Q \\cdot K)\\]\n\nOnce we get Attention to weights,\n\nwe multiply them by value vector (v_1, v_2, , v_N). So we have an updated embedding \\[\n\\bar{x}_i = \\sum w_{ij} x_j\n\\]\nHow to implement what we learned? How to do it from scratch? this is in the 3rd chapter of Natural Language Processing with Transformers with Hugging Face.\nI will cover it in the next blog . . . stay with me!!!!\nRESOURCES : (treat it as notes from CS224n and mentioned book )\n‚Üí Stanford Youtube CS224n and slides\n‚Üí Natural Language Processing with Transformers with Hugging Face Chapter 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bibhabasu Mohapatra",
    "section": "",
    "text": "I‚Äôm Bibhabasu Mohapatra, a Data Scientist at Microsoft with a passion for machine learning and computer vision.\n\n\n\nPersonal Website: bibhabasu-mohapatra.space"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Bibhabasu Mohapatra",
    "section": "",
    "text": "I‚Äôm Bibhabasu Mohapatra, a Data Scientist at Microsoft with a passion for machine learning and computer vision.\n\n\n\nPersonal Website: bibhabasu-mohapatra.space"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Bibhabasu‚Äôs Blog Page",
    "section": "",
    "text": "Hi there üëã This is my Blog Page - Though I wanted to create my personal site using Nextjs or React only, but some how I love blogging on quarto . . . now using sayak‚Äôs already available (which was used for my previous site, got back to blog on quarto) my site - bibhabasu-mohapatra.space\nI am Bibhabasu Mohapatra. Currently working as a Data Scientist at Microsoft, previously working as an Engineer(Computer Vision) at Aira Matrix.\nI‚Äôm particularly interested and want to work in topics related to self-supervised learning, multi-modal learning, and Representations \nOpen for Research Opportunities in Academia. for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Curve Fitting and doing Linear Regression from Scratch (!! Four ways to Solve a Least Squares Problem)\n\n\n\n\n\nLinear Regression from PRML Example 1 from Chapter 1 Introdcution also inspired from gilbert strang‚Äôs lecture on Four ways to Solve a Least Squares Problem\n\n\n\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize Salincy Maps in Timm Models (ViT) using PyTorch Hooks!!\n\n\n\n\n\nTutorial code of how I Visualize Attention Maps and Feature Maps (Salieancy Maps) in PyTorch timm using PyTorch Hooks as timm does not provide attention intermediates\n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding how EMA works?\n\n\n\n\n\nUnderstanding how EMA works with an simulation of a toy model with toy weights\n\n\n\n\n\nAug 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating Deep Supervision in Segmentation Models Pytorch (smp)?\n\n\n\n\n\nTutorial code of how I integrated Deep Supervision Loss generation setup with Segmentation Models PyTorch Framework, which could be Directly us with Minimal changes\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThresholding using scikit image (part of my kaggle submission code)\n\n\n\n\n\nWhen the noise gets growing, you should get out of the noise . . . to get a clear picture of what‚Äôs happening in your Image.\n\n\n\n\n\nJul 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStart with Transformers for Beginners\n\n\n\n\n\nWe Learnt ‚Äî What is Attention, Encoder + Decoder in RNNs, Attention in RNNs. Now we get to what is Self -Attention?\n\n\n\n\n\nJun 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage is Not a formal System, Language is glorious Chaos - Chris Manning (cs224n)\n\n\n\n\n\nfollowing the course Stanford CS224n and more . . . Take it as notes or tutorial, all credits to CS224n for this content I just mixed my understanding with my previous readings and intutions\n\n\n\n\n\nFeb 2, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "",
    "text": "In this Notebook - I share my understanding and my Work related to Deep Supervision in Models and Loss Function, I feel Deep Supervision is Sometimes Useful in the cases where the segmentation part bit hard to get detected.\n::: {#cell-2 .cell _cell_guid=‚Äòb1076dfc-b9ad-4769-8c92-a6c4dae69d19‚Äô _uuid=‚Äò8f2839f25d086af736a60e9eeb907d3b93b6e0e5‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2023-03-31T15:18:34.645310Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2023-03-31T15:18:34.645013Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2023-03-31T15:19:08.473332Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2023-03-31T15:19:08.471964Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2023-03-31T15:18:34.645281Z‚Äù}‚Äô trusted=‚Äòtrue‚Äô execution_count=1}\n:::"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#good-news-is-all-smps-and-timms-pretrained-weight-work-here",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#good-news-is-all-smps-and-timms-pretrained-weight-work-here",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "Good News is all SMP‚Äôs and timm‚Äôs Pretrained weight work here",
    "text": "Good News is all SMP‚Äôs and timm‚Äôs Pretrained weight work here\n\nENCODER = \"resnet18\"\nENCODER_WEIGHTS = \"imagenet\"\n\n\nOnly thing we need to take care is we must get Segmetation Model Class instead of smp.unet of library to use Deep Supervision\n\nmodel = SegmentationModel(encoder=ENCODER,encoder_weights=ENCODER_WEIGHTS)\n\n\ntrain_image = torch.randn((36,3,256,256),device=\"cuda\")\ntrain_labels = torch.ones((36,1,256,256),device=\"cuda\")\n\n\nmodel.to(\"cuda\")\nprediction = model(train_image)"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#output-is-a-list-whose-length-is-same-as-encoder-length-0th-index-is-the-main-index-of-prediction",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#output-is-a-list-whose-length-is-same-as-encoder-length-0th-index-is-the-main-index-of-prediction",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "Output is a List whose length is same as Encoder length 0‚Äôth index is the main Index of Prediction",
    "text": "Output is a List whose length is same as Encoder length 0‚Äôth index is the main Index of Prediction\n\nprint(\"look at the shape of Predictions\")\nfor out in prediction:\n    print(out.shape)\n\nlook at the shape of Predictions\ntorch.Size([36, 1, 256, 256])\ntorch.Size([36, 1, 128, 128])\ntorch.Size([36, 1, 64, 64])\ntorch.Size([36, 1, 32, 32])\ntorch.Size([36, 1, 16, 16])"
  },
  {
    "objectID": "posts/deep-supervised-in-segmentation-models-pytorch.html#implementation-output-of-loss-function",
    "href": "posts/deep-supervised-in-segmentation-models-pytorch.html#implementation-output-of-loss-function",
    "title": "Integrating Deep Supervision in Segmentation Models Pytorch (smp)?",
    "section": "Implementation output of Loss Function",
    "text": "Implementation output of Loss Function\n\nloss = Deep_Supervised_Loss()(prediction,train_labels)\nprint(loss.item())\n\n0.6200616359710693"
  },
  {
    "objectID": "posts/visualizing-vision-transformers-timm.html",
    "href": "posts/visualizing-vision-transformers-timm.html",
    "title": "Visualize Salincy Maps in Timm Models (ViT) using PyTorch Hooks!!",
    "section": "",
    "text": "This Blogs is a Part 2 of my Vision Transformer From Scratch Series, where I implement ViT from scratch. This Blog explores more on the Exaplainibility and visualizing the Saliency maps of any Transformer based Vision Model, It also delves into the concept of PyTorch Hooks which is used here to get the attention outputs from timm models. Where my basic understanding comes from Official Pytorch Hooks Documentation.\nalso credits : - https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\nalso available as youtube Video: - https://youtu.be/7q3NGMkEtjI?si=zRnBBIo5LEwrQM7e\n\nimport torch\nimport torch.nn as nn\n\nfrom skimage import io\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\n\nimg_path = \"/kaggle/input/image-net-visualization/bird_1.png\"\nmodel_path = \"vit_small_patch16_224.dino\"\nimage_size = 1024\n\n\ndef get_image(im_path, shape=image_size):\n    \n    \n    img = io.imread(im_path)[:,:,:3] ## HXWXC\n    H,W,C = img.shape\n    org_img = img.copy()\n    \n    img = A.Normalize()(image=img)[\"image\"] ## HxWxC\n    norm_img = img.copy()\n    \n    img = np.transpose(img,(2,0,1)) ## CxHxW\n    img = np.expand_dims(img,0) ## B=1,CxHxW\n    \n    img = torch.tensor(img)\n    img = nn.Upsample((shape,shape),mode='bilinear')(img)\n    \n    return img, norm_img, org_img\n\n\nimg,norm_img,org_img = get_image(img_path)\n\n\nio.imshow(org_img)\n\n\n\n\n\n\n\n\n\nio.imshow(norm_img)\n\n\n\n\n\n\n\n\n\n%%capture\n! pip install timm\n\n\nimport timm\nmodel = timm.create_model(model_path,pretrained=True,\n                         img_size=image_size,\n                         dynamic_img_size=True)\n\n\n\n\n\nmodel = model.cuda()\n\n\noutputs = {}\ndef get_outputs(name:str):\n    def hook(model, input, output):\n        outputs[name] = output.detach()\n        \n    return hook\n\n\nmodel.blocks[-1].attn.q_norm.register_forward_hook(get_outputs('Q'))\nmodel.blocks[-1].attn.k_norm.register_forward_hook(get_outputs('K'))\nmodel.blocks[-1].register_forward_hook(get_outputs('features'))\n\nmodel(img.cuda())\n\ntensor([[-1.6873e+00,  3.3118e+00,  1.6206e-02,  7.5276e+00, -3.8783e+00,\n          7.9405e-01, -2.5275e-01, -9.2937e-01, -7.6219e+00,  1.8461e+00,\n         -3.7896e+00,  6.9062e+00,  4.9580e+00,  7.0582e+00, -1.4598e+00,\n          1.8604e+00,  3.1002e+00, -2.3646e-01,  9.7898e-01,  1.4716e-01,\n          4.8010e+00,  9.8696e+00, -6.2256e+00, -4.0314e+00,  4.8572e-01,\n          4.6386e+00,  5.2622e+00,  8.2102e+00,  6.6765e+00, -1.0041e+01,\n          9.1688e-01,  2.6036e+00,  1.0652e+00, -1.0549e+01, -1.5771e+00,\n         -2.4766e+00, -4.6636e+00,  5.6768e+00, -1.7559e+00, -4.9503e-01,\n         -2.5628e+00,  1.0277e+01, -4.6592e+00, -3.6269e+00,  6.4073e+00,\n         -2.8656e+00, -8.2731e+00,  9.1952e+00, -2.4074e+00, -5.6380e-01,\n         -3.4731e+00, -4.7151e+00,  1.8822e-01, -3.8197e+00,  2.5455e+00,\n         -1.3325e-01, -4.3335e+00,  7.5765e+00, -3.5712e+00, -4.4365e+00,\n          4.9343e-01,  9.1502e-01,  1.0584e+01,  3.9611e+00,  3.8866e+00,\n         -4.6030e+00,  1.4714e+00,  4.2984e-01, -3.4557e+00,  1.0801e+00,\n         -4.1478e+00,  5.2226e+00,  2.8259e+00,  9.7525e-01,  6.9041e+00,\n          1.2095e-01,  2.4013e+00,  1.6391e+00, -6.0108e+00, -9.3346e+00,\n         -4.0121e+00,  1.8157e+00, -7.2864e-01,  8.5839e+00, -1.0082e+01,\n          1.2923e+00, -1.8654e+00, -1.0347e+01,  4.6857e+00, -2.7471e+00,\n          3.4723e+00,  4.6032e+00, -2.5349e+00, -2.0960e+00,  1.6807e-01,\n          1.9170e+00, -6.1443e+00,  1.0321e+00, -1.6014e+00,  5.8918e+00,\n          9.2721e+00, -1.2786e+00, -6.3613e+00,  5.5546e+00, -1.2483e+01,\n          3.0828e+00, -6.0050e-02, -1.3473e+01,  6.6318e-01, -5.3011e+00,\n         -3.5312e+00, -1.5129e+00,  4.1372e+00,  1.4356e+00,  1.0351e+01,\n         -1.7940e+00, -1.2211e+01,  2.8142e+00, -1.5071e+00, -6.9505e-01,\n         -3.1786e+00,  7.8665e+00,  6.7557e-01,  2.6826e+00, -2.4210e+00,\n         -2.8962e+00,  5.5639e+00, -1.8775e+00, -1.2859e+01, -7.5849e-01,\n          1.6650e+00,  2.6447e+00, -6.3936e-01,  1.5060e+00,  6.0126e-01,\n          3.9601e+00, -1.8724e+00, -3.1135e+00, -2.8265e+00,  1.6418e+00,\n         -5.6146e+00, -9.4629e-02, -1.3282e-01,  7.6737e+00, -1.5090e+00,\n          5.6526e+00, -9.7128e-01, -3.6167e+00,  1.7872e+00, -2.7470e+00,\n         -2.4107e+00, -7.7301e-03,  5.4260e+00, -2.6830e-01, -6.0379e-01,\n          2.4405e+00, -3.0422e+00, -6.1176e+00, -1.3285e+00, -1.1136e+01,\n         -3.1303e+00,  2.8008e+00,  2.7368e+00, -2.8170e+00, -4.1028e+00,\n          5.4076e+00,  1.6082e+00, -9.9726e-01,  5.4990e+00,  1.8774e+00,\n         -5.4387e-01, -2.9433e-01,  3.6973e+00, -4.5119e+00, -3.5505e+00,\n          2.8779e+00,  3.0147e-01, -2.3486e+00, -1.5237e+00,  1.1389e+00,\n          2.2250e+00,  6.8706e+00,  3.7546e+00, -1.6722e+00, -6.4871e-01,\n          3.2394e+00,  6.6144e-01,  1.0893e+00,  1.1054e+00,  4.0759e+00,\n         -4.9504e+00, -4.0154e+00, -1.4490e+00,  1.8689e+00, -3.9057e+00,\n          2.2533e+00, -6.3996e+00, -5.5835e-01,  2.4417e+00,  2.1308e+00,\n          3.9925e+00,  4.4550e-01,  1.1531e+00,  4.7416e+00,  2.0096e+00,\n         -1.2605e+00, -9.5855e-01, -5.2844e+00, -4.0194e+00,  7.0700e-01,\n          5.4294e+00,  3.3919e+00,  4.9089e+00,  4.1371e-01,  2.2920e+00,\n         -1.7367e-01, -4.7534e+00,  5.0581e+00,  8.1151e+00, -1.5215e+00,\n          3.7808e+00, -2.9530e+00, -9.3015e+00,  6.1947e-01, -3.9256e-01,\n         -4.1782e+00,  1.9628e+00,  6.7975e+00,  1.2619e+00,  1.7882e+00,\n         -9.8904e-01, -3.4642e-01, -2.1700e+00,  8.9316e+00,  4.0332e+00,\n         -3.3242e+00,  3.7758e+00,  2.3272e+00, -2.6639e+00,  1.8211e+00,\n          2.0843e+00,  9.2511e+00,  7.0950e-01,  2.3567e+00, -3.4504e+00,\n         -1.2123e+01, -2.2370e-01, -4.0639e+00, -7.6606e-01, -5.7467e-01,\n          6.2722e+00, -1.1846e+00,  7.4569e-01, -2.3022e+00, -7.6787e+00,\n          1.5879e+00, -5.4248e+00,  4.4352e+00, -3.0992e-01, -9.8239e+00,\n          4.7596e+00, -7.8933e+00, -1.5991e+00,  3.5556e+00, -8.2829e+00,\n         -2.8837e+00,  8.0843e+00,  4.4499e+00, -7.8938e+00,  1.1607e+01,\n          3.3790e+00, -1.0773e+01,  4.5276e+00,  2.0844e+00,  2.4876e+00,\n          2.4884e+00,  2.1223e-01, -3.0398e+00, -6.3939e+00, -2.2402e+00,\n         -9.1261e-01,  2.2716e+00,  7.5233e+00,  3.2600e+00,  1.3579e+00,\n          3.0965e+00, -1.2645e+00,  3.8311e-01, -3.8111e+00, -1.9618e+00,\n          2.7574e+00,  1.0933e+01, -8.1753e+00, -5.7976e-01,  4.3519e+00,\n         -3.1721e+00,  3.5473e+00,  8.9204e-01, -5.8862e+00,  5.9441e+00,\n         -4.1584e+00,  2.6372e+00,  5.9613e-01, -3.5491e+00, -7.7043e+00,\n         -5.9683e+00, -1.9161e-01, -3.4110e+00, -4.9706e+00,  3.6422e+00,\n          2.0380e+00, -1.7211e-01,  2.2398e+00, -8.1027e-01, -1.7448e+00,\n         -8.5672e-01, -3.8151e-02, -1.5993e+00,  3.6409e+00,  3.0737e+00,\n          1.5809e-01, -2.5176e-01,  3.4933e+00, -6.0633e+00,  6.0766e+00,\n          6.8786e+00, -2.1457e+00,  4.7027e-01, -5.3296e-01, -9.8374e+00,\n         -4.7503e+00, -1.6798e+01,  5.6179e+00,  1.0797e+00,  9.5722e-01,\n          1.0259e+01,  2.4940e+00,  5.1237e+00, -3.9690e+00,  6.3755e+00,\n         -2.4782e+00,  5.0692e+00,  1.1048e+00, -3.4466e+00, -8.6174e+00,\n         -1.3207e+00, -1.0411e+01, -1.8757e+00, -1.1057e+01, -5.9575e+00,\n          1.0189e+00,  2.0985e+00, -3.2184e+00, -3.6003e+00, -4.3049e+00,\n          3.8259e+00, -4.0951e+00, -1.8868e+00, -4.7576e+00, -6.7396e+00,\n          6.9816e-01, -2.2439e+00, -4.5493e+00,  8.9310e+00, -1.6068e-01,\n         -2.5645e+00,  7.0637e+00,  1.7403e+01,  2.9385e+00,  1.5559e+00,\n          5.3462e+00,  1.3728e+01,  3.9456e+00, -5.9510e+00,  3.1551e+00,\n          4.6709e+00, -3.6971e+00, -1.2921e+00,  2.9455e+00,  4.7313e+00,\n         -2.5174e+00,  6.0789e+00, -7.3726e+00, -2.1013e+00]], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nscale = model.blocks[-1].attn.scale\n\n\noutputs[\"attention\"] = (outputs[\"Q\"]@ outputs[\"K\"].transpose(-2, -1))#*scale\n# outputs[\"attention\"] = outputs[\"attention\"].softmax(dim=-1)\n\n\noutputs[\"attention\"].shape\n\ntorch.Size([1, 6, 4097, 4097])\n\n\n\nb,num_heads,num_patches_1,num_patches_1 = outputs[\"attention\"].shape\nmap_size = int(np.sqrt(num_patches_1-1))\nfor attention_head in range(num_heads):\n    attention_map = outputs[\"attention\"][:,attention_head,0,1:] ## 1,4096\n    attention_map = attention_map.view(1,1,map_size,map_size)\n    \n    attention_map = nn.Upsample(size=(image_size,image_size))(attention_map)\n    \n    attention_map = attention_map[0,0,:,:].detach().cpu().numpy()\n    \n    io.imshow(attention_map)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutputs[\"features\"].shape\n\ntorch.Size([1, 4097, 384])\n\n\n\nfeatures = outputs[\"features\"].mean(-1)\nfeatures = features[:,1:]\n\nfeatures = features.view(1,1,map_size,map_size)\n\nfeatures = nn.Upsample(size=(image_size,image_size))(features)\n\nfeatures = features[0,0,:,:].detach().cpu().numpy()\n\nio.imshow(features)\n\n/opt/conda/lib/python3.10/site-packages/skimage/io/_plugins/matplotlib_plugin.py:149: UserWarning: Low image data range; displaying image with stretched contrast.\n  lo, hi, cmap = _get_display_range(image)"
  },
  {
    "objectID": "posts/understanding-how-ema-works.html",
    "href": "posts/understanding-how-ema-works.html",
    "title": "Understanding how EMA works?",
    "section": "",
    "text": "EMA (Exponential Moving Average) is an incredibly useful concept that finds application in various scenarios:\n\nWeight Updates: EMA is used for updating model weights while retaining a historical record of previous weights. This enables the model to blend new information with past knowledge effectively.\nSelf-Supervised Learning: EMA is commonly employed in Self-Supervised Learning setups. The weights obtained from Self-Supervised Learning are often utilized for downstream tasks like classification and segmentation.\n\n\n\nInitially, there was a misconception that the EMA process directly impacts the ongoing training of model weights. However, this is not the case. In reality, the EMA process involves the creation of a duplicated set of weights. These duplicate weights are updated alongside the primary training process, and the updated weights are subsequently leveraged for validation purposes. As a result, the overall training procedure remains unaffected by the EMA process.\n\n\n\nimage.png\n\n\n::: {#36edc225 .cell _cell_guid=‚Äòb1076dfc-b9ad-4769-8c92-a6c4dae69d19‚Äô _uuid=‚Äò8f2839f25d086af736a60e9eeb907d3b93b6e0e5‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2023-08-14T15:14:27.136587Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2023-08-14T15:14:27.136194Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2023-08-14T15:14:30.874853Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2023-08-14T15:14:30.873447Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:3.746175,‚Äúend_time‚Äù:‚Äú2023-08-14T15:14:30.877588‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2023-08-14T15:14:27.131413‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[]‚Äô execution_count=1}\nimport torch.nn as nn\nimport torch\nfrom copy import deepcopy\n\nclass Model(nn.Module):\n    def __init__(self,):\n        super().__init__()\n\n        self.layer = nn.Linear(3,3,bias=False)\n\n    def forward(self,x):\n        return self.layer(x)\n    \n@torch.no_grad()\ndef init_weights_1(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n@torch.no_grad()\ndef init_weights_2(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(2.0)\n        print(m.weight)\n:::\n\nteacher = Model().apply(init_weights_1) ## TO BE UPDATED\nstudent = Model().apply(init_weights_2) ## From Training\n\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\n\n\n\n@torch.no_grad()\ndef ema_model(teacher_model, student_model,decay=0.5):\n    teacher_model.eval()\n\n    for teacher_wt,student_wt in zip(teacher_model.state_dict().values(),student_model.state_dict().values()):\n        teacher_wt.copy_(teacher_wt*decay + student_wt*(1-decay))\n\n\nema_model(teacher,student)\n\n\nteacher.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000]]))])\n\n\n\nstudent.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[2., 2., 2.],\n                      [2., 2., 2.],\n                      [2., 2., 2.]]))])"
  },
  {
    "objectID": "posts/understanding-how-ema-works.html#exponential-moving-average-ema-in-weight-updates",
    "href": "posts/understanding-how-ema-works.html#exponential-moving-average-ema-in-weight-updates",
    "title": "Understanding how EMA works?",
    "section": "",
    "text": "EMA (Exponential Moving Average) is an incredibly useful concept that finds application in various scenarios:\n\nWeight Updates: EMA is used for updating model weights while retaining a historical record of previous weights. This enables the model to blend new information with past knowledge effectively.\nSelf-Supervised Learning: EMA is commonly employed in Self-Supervised Learning setups. The weights obtained from Self-Supervised Learning are often utilized for downstream tasks like classification and segmentation.\n\n\n\nInitially, there was a misconception that the EMA process directly impacts the ongoing training of model weights. However, this is not the case. In reality, the EMA process involves the creation of a duplicated set of weights. These duplicate weights are updated alongside the primary training process, and the updated weights are subsequently leveraged for validation purposes. As a result, the overall training procedure remains unaffected by the EMA process.\n\n\n\nimage.png\n\n\n::: {#36edc225 .cell _cell_guid=‚Äòb1076dfc-b9ad-4769-8c92-a6c4dae69d19‚Äô _uuid=‚Äò8f2839f25d086af736a60e9eeb907d3b93b6e0e5‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2023-08-14T15:14:27.136587Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2023-08-14T15:14:27.136194Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2023-08-14T15:14:30.874853Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2023-08-14T15:14:30.873447Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:3.746175,‚Äúend_time‚Äù:‚Äú2023-08-14T15:14:30.877588‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2023-08-14T15:14:27.131413‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[]‚Äô execution_count=1}\nimport torch.nn as nn\nimport torch\nfrom copy import deepcopy\n\nclass Model(nn.Module):\n    def __init__(self,):\n        super().__init__()\n\n        self.layer = nn.Linear(3,3,bias=False)\n\n    def forward(self,x):\n        return self.layer(x)\n    \n@torch.no_grad()\ndef init_weights_1(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n@torch.no_grad()\ndef init_weights_2(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(2.0)\n        print(m.weight)\n:::\n\nteacher = Model().apply(init_weights_1) ## TO BE UPDATED\nstudent = Model().apply(init_weights_2) ## From Training\n\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\n\n\n\n@torch.no_grad()\ndef ema_model(teacher_model, student_model,decay=0.5):\n    teacher_model.eval()\n\n    for teacher_wt,student_wt in zip(teacher_model.state_dict().values(),student_model.state_dict().values()):\n        teacher_wt.copy_(teacher_wt*decay + student_wt*(1-decay))\n\n\nema_model(teacher,student)\n\n\nteacher.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000]]))])\n\n\n\nstudent.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[2., 2., 2.],\n                      [2., 2., 2.],\n                      [2., 2., 2.]]))])"
  }
]