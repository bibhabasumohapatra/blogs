[
  {
    "objectID": "posts/linear-regression-from-scratch.html",
    "href": "posts/linear-regression-from-scratch.html",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "import torch\n\n\nx_train = torch.linspace(0, 1, 20)\ny_train = torch.sin(2*3.14*x_train) + torch.normal(0,0.2,x_train.shape,) ## added noise as per the book\n\n### testing data\nx_test = torch.linspace(0, 1, 200)\ny_test = torch.sin(2*3.14*x_test) \n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\nTo solve for the optimal weight matrix ( W ), given the expression ( (XW - Y)^T (XW - Y) ), we can proceed by first expanding the expression and then minimizing it with respect to ( W ).\n\n\nGiven: \\[\nL(W) = (XW - Y)^T (XW - Y)\n\\]\nExpanding this: \\[\nL(W) = (XW)^T(XW) - (XW)^T Y - Y^T (XW) + Y^T Y\n\\]\nSimplifying further: \\[\nL(W) = W^T X^T XW - W^T X^T Y - Y^T XW + Y^T Y\n\\]\nSince ( W^T X^T Y ) and ( Y^T XW ) are scalars, they are equal. Therefore: \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTo find the optimal ( W ), take the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2X^T XW - 2X^T Y = 0\n\\]\nSimplifying: \\[\nX^T XW = X^T Y\n\\]\nFinally, solve for ( W ): \\[\nW = (X^T X)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is given by: \\[\n\\boxed{W = (X^T X)^{-1} X^T Y}\n\\]\nThis result is commonly known as the Normal Equation in linear regression.\n\npowers = torch.arange(5).float()\nx_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\nw = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n\n\nw.shape, x_test.shape\n\n(torch.Size([5, 1]), torch.Size([200]))\n\n\n\nx_poly_test = x_test.unsqueeze(-1).pow(powers)\ny_pred = x_poly_test @ w\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\nplt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    w = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))\n\n\n\n\n\n\n\n\n\n\n\n\nOne technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified error function of the form.\nTechniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefficients. The particular case of a quadratic regularizer is called ridge regres\u0002sion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is known as weight decay\nTo solve for the optimal weight matrix ( W ) given the objective function ( (XW - Y)^T (XW - Y) + W^T W ), follow these steps:\n\n\nThe objective function is: \\[\nL(W) = (XW - Y)^T (XW - Y) + W^T W\n\\]\n\n\n\nExpanding ( (XW - Y)^T (XW - Y) ): \\[\n(XW - Y)^T (XW - Y) = (XW)^T XW - (XW)^T Y - Y^T XW + Y^T Y\n\\]\nSince ( (XW)^T Y ) and ( Y^T XW ) are scalars and equal, this simplifies to: \\[\n(XW - Y)^T (XW - Y) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\nAdding ( W^T W ): \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y + W^T W\n\\]\nCombining like terms: \\[\nL(W) = W^T (X^T X + I) W - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTake the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2(X^T X + I)W - 2X^T Y = 0\n\\]\nSolving for ( W ): \\[\n(X^T X + I)W = X^T Y\n\\] \\[\nW = (X^T X + I)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is: \\[\n\\boxed{W = (X^T X + I)^{-1} X^T Y}\n\\]\nThis result includes a regularization term ( W^T W ), which is common in Ridge Regression or Tikhonov Regularization.\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    N,M = x_poly.shape\n\n    w = (torch.linalg.inv(x_poly.T@x_poly + torch.eye(M))@x_poly.T) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#plot-training-data",
    "href": "posts/linear-regression-from-scratch.html#plot-training-data",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.scatter(x_train, y_train)"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#fit-the-linear-model-on-training-data",
    "href": "posts/linear-regression-from-scratch.html#fit-the-linear-model-on-training-data",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "To solve for the optimal weight matrix ( W ), given the expression ( (XW - Y)^T (XW - Y) ), we can proceed by first expanding the expression and then minimizing it with respect to ( W ).\n\n\nGiven: \\[\nL(W) = (XW - Y)^T (XW - Y)\n\\]\nExpanding this: \\[\nL(W) = (XW)^T(XW) - (XW)^T Y - Y^T (XW) + Y^T Y\n\\]\nSimplifying further: \\[\nL(W) = W^T X^T XW - W^T X^T Y - Y^T XW + Y^T Y\n\\]\nSince ( W^T X^T Y ) and ( Y^T XW ) are scalars, they are equal. Therefore: \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTo find the optimal ( W ), take the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2X^T XW - 2X^T Y = 0\n\\]\nSimplifying: \\[\nX^T XW = X^T Y\n\\]\nFinally, solve for ( W ): \\[\nW = (X^T X)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is given by: \\[\n\\boxed{W = (X^T X)^{-1} X^T Y}\n\\]\nThis result is commonly known as the Normal Equation in linear regression.\n\npowers = torch.arange(5).float()\nx_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\nw = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n\n\nw.shape, x_test.shape\n\n(torch.Size([5, 1]), torch.Size([200]))\n\n\n\nx_poly_test = x_test.unsqueeze(-1).pow(powers)\ny_pred = x_poly_test @ w\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\nplt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    w = torch.linalg.pinv(x_poly) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "posts/linear-regression-from-scratch.html#example-polynomial-curve-fitting-with-regularization",
    "href": "posts/linear-regression-from-scratch.html#example-polynomial-curve-fitting-with-regularization",
    "title": "Linear Regression from Scratch",
    "section": "",
    "text": "One technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified error function of the form.\nTechniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefficients. The particular case of a quadratic regularizer is called ridge regres\u0002sion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is known as weight decay\nTo solve for the optimal weight matrix ( W ) given the objective function ( (XW - Y)^T (XW - Y) + W^T W ), follow these steps:\n\n\nThe objective function is: \\[\nL(W) = (XW - Y)^T (XW - Y) + W^T W\n\\]\n\n\n\nExpanding ( (XW - Y)^T (XW - Y) ): \\[\n(XW - Y)^T (XW - Y) = (XW)^T XW - (XW)^T Y - Y^T XW + Y^T Y\n\\]\nSince ( (XW)^T Y ) and ( Y^T XW ) are scalars and equal, this simplifies to: \\[\n(XW - Y)^T (XW - Y) = W^T X^T XW - 2W^T X^T Y + Y^T Y\n\\]\nAdding ( W^T W ): \\[\nL(W) = W^T X^T XW - 2W^T X^T Y + Y^T Y + W^T W\n\\]\nCombining like terms: \\[\nL(W) = W^T (X^T X + I) W - 2W^T X^T Y + Y^T Y\n\\]\n\n\n\nTake the derivative of ( L(W) ) with respect to ( W ) and set it to zero: \\[\n\\frac{\\partial L(W)}{\\partial W} = 2(X^T X + I)W - 2X^T Y = 0\n\\]\nSolving for ( W ): \\[\n(X^T X + I)W = X^T Y\n\\] \\[\nW = (X^T X + I)^{-1} X^T Y\n\\]\n\n\n\nThe optimal weight matrix ( W ) is: \\[\n\\boxed{W = (X^T X + I)^{-1} X^T Y}\n\\]\nThis result includes a regularization term ( W^T W ), which is common in Ridge Regression or Tikhonov Regularization.\n\nfor i, degree in enumerate([0, 1, 3, 9]):\n    plt.subplot(2, 2, i + 1)\n    powers = torch.arange(degree).float()\n    x_poly = x_train.unsqueeze(-1).pow(powers) ## NxM\n    N,M = x_poly.shape\n\n    w = (torch.linalg.inv(x_poly.T@x_poly + torch.eye(M))@x_poly.T) @ y_train.view(-1,1) ## MxN * Nx1 == Mx1\n    x_poly_test = x_test.unsqueeze(-1).pow(powers)\n    y_pred = x_poly_test @ w    \n\n    plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n    plt.plot(x_test, y_pred, c=\"r\", label=\"fitting\")\n    plt.ylim(-1.5, 1.5)\n    plt.annotate(\"M={}\".format(degree), xy=(-0.15, 1))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bibhabasu’s Blog Page",
    "section": "",
    "text": "Hi there 👋 This is my Blog Page - Though I wanted to creat my personal site using Nextjs or React only, but some how I love blogging on quarto . . . now using sayak’s already available (which used for my previous site, got back to blog on quarto) my site - bibhabasu-mohapatra.space\nI am Bibhabasu Mohapatra. Currently working as a Data Scientist at Microsoft, previously working as an Engineer(Computer Vision) at Aira Matrix.\nI’m particularly interested and want to work in topics related to self-supervised learning, multi-modal learning, and Representations \nOpen for Research Opportunities in Academia. for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression from Scratch\n\n\n\n\n\nLinear Regression from PRML Example 1 from Chapter 1 Introdcution\n\n\n\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding how EMA works?\n\n\n\n\n\nUnderstanding how EMA works with an simulation of a toy model with toy weights\n\n\n\n\n\nAug 14, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/understanding-how-ema-works.html",
    "href": "posts/understanding-how-ema-works.html",
    "title": "Understanding how EMA works?",
    "section": "",
    "text": "EMA (Exponential Moving Average) is an incredibly useful concept that finds application in various scenarios:\n\nWeight Updates: EMA is used for updating model weights while retaining a historical record of previous weights. This enables the model to blend new information with past knowledge effectively.\nSelf-Supervised Learning: EMA is commonly employed in Self-Supervised Learning setups. The weights obtained from Self-Supervised Learning are often utilized for downstream tasks like classification and segmentation.\n\n\n\nInitially, there was a misconception that the EMA process directly impacts the ongoing training of model weights. However, this is not the case. In reality, the EMA process involves the creation of a duplicated set of weights. These duplicate weights are updated alongside the primary training process, and the updated weights are subsequently leveraged for validation purposes. As a result, the overall training procedure remains unaffected by the EMA process.\n\n\n\nimage.png\n\n\n::: {#36edc225 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-08-14T15:14:27.136587Z”,“iopub.status.busy”:“2023-08-14T15:14:27.136194Z”,“iopub.status.idle”:“2023-08-14T15:14:30.874853Z”,“shell.execute_reply”:“2023-08-14T15:14:30.873447Z”}’ papermill=‘{“duration”:3.746175,“end_time”:“2023-08-14T15:14:30.877588”,“exception”:false,“start_time”:“2023-08-14T15:14:27.131413”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport torch.nn as nn\nimport torch\nfrom copy import deepcopy\n\nclass Model(nn.Module):\n    def __init__(self,):\n        super().__init__()\n\n        self.layer = nn.Linear(3,3,bias=False)\n\n    def forward(self,x):\n        return self.layer(x)\n    \n@torch.no_grad()\ndef init_weights_1(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n@torch.no_grad()\ndef init_weights_2(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(2.0)\n        print(m.weight)\n:::\n\nteacher = Model().apply(init_weights_1) ## TO BE UPDATED\nstudent = Model().apply(init_weights_2) ## From Training\n\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\n\n\n\n@torch.no_grad()\ndef ema_model(teacher_model, student_model,decay=0.5):\n    teacher_model.eval()\n\n    for teacher_wt,student_wt in zip(teacher_model.state_dict().values(),student_model.state_dict().values()):\n        teacher_wt.copy_(teacher_wt*decay + student_wt*(1-decay))\n\n\nema_model(teacher,student)\n\n\nteacher.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000]]))])\n\n\n\nstudent.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[2., 2., 2.],\n                      [2., 2., 2.],\n                      [2., 2., 2.]]))])"
  },
  {
    "objectID": "posts/understanding-how-ema-works.html#exponential-moving-average-ema-in-weight-updates",
    "href": "posts/understanding-how-ema-works.html#exponential-moving-average-ema-in-weight-updates",
    "title": "Understanding how EMA works?",
    "section": "",
    "text": "EMA (Exponential Moving Average) is an incredibly useful concept that finds application in various scenarios:\n\nWeight Updates: EMA is used for updating model weights while retaining a historical record of previous weights. This enables the model to blend new information with past knowledge effectively.\nSelf-Supervised Learning: EMA is commonly employed in Self-Supervised Learning setups. The weights obtained from Self-Supervised Learning are often utilized for downstream tasks like classification and segmentation.\n\n\n\nInitially, there was a misconception that the EMA process directly impacts the ongoing training of model weights. However, this is not the case. In reality, the EMA process involves the creation of a duplicated set of weights. These duplicate weights are updated alongside the primary training process, and the updated weights are subsequently leveraged for validation purposes. As a result, the overall training procedure remains unaffected by the EMA process.\n\n\n\nimage.png\n\n\n::: {#36edc225 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-08-14T15:14:27.136587Z”,“iopub.status.busy”:“2023-08-14T15:14:27.136194Z”,“iopub.status.idle”:“2023-08-14T15:14:30.874853Z”,“shell.execute_reply”:“2023-08-14T15:14:30.873447Z”}’ papermill=‘{“duration”:3.746175,“end_time”:“2023-08-14T15:14:30.877588”,“exception”:false,“start_time”:“2023-08-14T15:14:27.131413”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport torch.nn as nn\nimport torch\nfrom copy import deepcopy\n\nclass Model(nn.Module):\n    def __init__(self,):\n        super().__init__()\n\n        self.layer = nn.Linear(3,3,bias=False)\n\n    def forward(self,x):\n        return self.layer(x)\n    \n@torch.no_grad()\ndef init_weights_1(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n@torch.no_grad()\ndef init_weights_2(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(2.0)\n        print(m.weight)\n:::\n\nteacher = Model().apply(init_weights_1) ## TO BE UPDATED\nstudent = Model().apply(init_weights_2) ## From Training\n\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\n\n\n\n@torch.no_grad()\ndef ema_model(teacher_model, student_model,decay=0.5):\n    teacher_model.eval()\n\n    for teacher_wt,student_wt in zip(teacher_model.state_dict().values(),student_model.state_dict().values()):\n        teacher_wt.copy_(teacher_wt*decay + student_wt*(1-decay))\n\n\nema_model(teacher,student)\n\n\nteacher.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000]]))])\n\n\n\nstudent.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[2., 2., 2.],\n                      [2., 2., 2.],\n                      [2., 2., 2.]]))])"
  }
]